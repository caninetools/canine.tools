---
date: 2025-04-27
title: "Anubis Report #1"
description: "Statistics from using Anubis for over 2 weeks."
---
Over 2 weeks ago, we deployed Anubis to stop bot attacks and scraping. Here are the results!
<!-- more -->

<figure markdown="span">
  ![Anubis Mascot](https://raw.githubusercontent.com/TecharoHQ/anubis/refs/heads/main/web/static/img/happy.webp){ width=60% }
</figure>

## What is Anubis?
Anubis is a bot/AI scraper protection challenge. It sits in front of most frontends & services we host to protect the mass scraping of data.

Anubis [weighs the soul of your connection](https://en.wikipedia.org/wiki/Weighing_of_souls). You can read more about how Anubis works on [their site](https://anubis.techaro.lol/docs/design/how-anubis-works) (I am too dumb to explain it).

## Why is it needed?
I deployed Anubis after discovering it over a month ago as of writing this post. Since we don't use Cloudflare, the only way we can stop bots from scraping or sending annoying requests is through our web server.

We did this for a long, long time. However, this simply told NGINX to block certain user agents. We all know they can be faked, so NGINX allowed most of these bots.

I didn't really like doing this, and I wanted a more long term solution. After finding Anubis, I instantly deployed it.

I've seen *several* bots hitting many different of our frontends over the months of running the canine.tools project. We host alternative "frontends" to different websites. These have a bunch of different features, you can see which ones we host on our [services page](https://canine.tools/services/). Most of these proxy all requests, don't track you, and they are much faster.

Here's some examples of "attacks" we've seen:

* [Sending random text to our mozhi instance](https://canine.tools/blog/2024/09/16/september-16-2024/)
* [Scraping search results from our SearXNG instance](https://canine.tools/blog/2024/11/05/november-5-2024/)
* Scarping things from our Priviblur instance

Overall, I really don't like these bots using our frontends for this purpose. I originally allowed it, but it became too much to handle. Furthermore, these bots would get blocked on the real site, so our frontend is an easy way to not get blocked.

## Stats
2 weeks ago I decided to log Anubis to track how many requests we get. This logs how many times it challenges or denies users. *This log is going to be wiped after this blog post, do not worry. I am not tracking you. This is all for statistics.*

### Network Usage
To start, let's look at the network data from March of 2025.

<figure markdown="span">
  ![Monthly Network Data](https://canine.tools/assets/images/blog/16/monthly-network.png)
</figure>

As you can see, we have almost 4TiB of total receive and transfer network data for March. It's the end of April, and we have 2.45TiB total. Anubis has been deployed since March 31st. As you can see, there is a drop.

This increase has been present every month, *so I suspect this month is lower due to Anubis being deployed.*

### Total Traffic
Since I tracked Anubis data for 2 weeks now, let's take a look at the results.

* **Total Challenges: 49,446** - how many times a user's browser was challenged.
* **Blocked Bots: 523** - how many times Anubis blocked something.

That means Anubis blocked 1.06% of all requests. But why does the network usage not reflect this? This doesn't make sense.

My guess is Anubis blocked a bot, and the bot stopped for a few hours and tried again, only to be blocked once more. Furthermore, I added fail2ban support to block all connections if an IP fails for a few hours. I believe this impact alone has resulted in the drop of network usage for this month.

### Bot Data
Let's take a look at what Anubis blocked.

1. Headless Chrome - 367
2. Bytespider - 150
3. AI2Bot - 3
4. Scrapy - 2
5. GPTBot - 1

To my surprise, the biggest "bot" was [Headless Chrome](https://developer.chrome.com/blog/headless-chrome/). My guess is these bots try to use a real browser engine to make it look like they are real, but forgot to change the user agent. 2nd place was Bytespider, which I have seen hit one service specifically (listed below). The other ones were random AI/scrapers that found my site.

Here's what services were hit the most:

1. [search.canine.tools](https://search.canine.tools) - 338
2. [tent.canine.tools](https://tent.canine.tools) - 135
3. [canine.tools](https://canine.tools) - 36
4. [priviblur.canine.tools](https://priviblur.canine.tools) - 6
5. [overflow.canine.tools](https://overflow.canine.tools) - 3
6. [redlib.canine.tools](https://redlib.canine.tools) - 3
7. [quetre.canine.tools](https://quetre.canine.tools) - 3

As stated earlier in this post, our SearXNG instance was hit hard a few months ago and still is. It makes sense to scrape data from it, as it's a search engine.

## Conclusion
Anubis has really helped our site block these annoying bots/scrapers. I am looking forward to seeing what it develops into in the future! Please support Anubis. The canine.tools project sponsors the developer on GitHub, [and you should too](https://github.com/sponsors/Xe)!